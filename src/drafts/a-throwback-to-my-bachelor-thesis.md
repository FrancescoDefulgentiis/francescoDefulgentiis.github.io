---
title: "Squeezing Uncertainty: Compressing Covariance on the Edge"
date: 2026-02-07
description: how to make a NAO robot see the world without melting its processor.
toRead: 10
---
---
I’ve been stuck in a mental backlog recently, thinking about constraints. We live in an era of infinite cloud compute, but down on the metal—on the edge—resources are still scarce.

I (not so) recently dove into a project involving the SPQR team at Sapienza University, specifically looking at the RoboCup Open Soccer League. The platform is the NAO robot. It’s a standard bipedal hardware shared by all teams, which means you can’t buy your way to victory with better sensors. You have to code better.

The problem? These robots have embedded processors with limited performance and internal memory. Yet, they need to track obstacles (other robots, goalposts) in the space around them, projecting them into a 2D plane and share that data with the team.

We aren't just tracking position $(X, Y)$; we are tracking the *uncertainty* of that position; the covariance matrix. Storing and transmitting full float matrices for every obstacle creates a bottleneck. I wanted to see if we could crush this data down without making the robot blind.

## The Signal in the Noise

First, I had to look at what we were actually dealing with. A covariance matrix for a 2D obstacle usually involves $X$, $Y$, and $\theta$ (orientation).

> I ran the simulation logs and found a ghost in the machine.

When analyzing the data generated by SimRobot , the variance for $\theta$ (orientation) was consistently near zero. The robot knew which way the obstacle was facing, or it didn't matter enough to vary.

So, `git reset --hard`. We drop $\theta$.

Since covariance matrices are symmetric, a $2 \times 2$ matrix only has three unique values that matter. We don't need to send the whole block. We just need to compress three numbers.

## Three Paths to Compression

I explored three different architectures to solve this. It’s a classic engineering trade-off triangle: Speed, Accuracy, Memory. Pick two.

### 1. The Mathematical Hack: Base Conversion

The idea here is raw mapping. We treat the tuple of three values $(x_1, x_2, x_3)$ as digits in a base-32 system. It fits into a `short` integer.

```python
# Concept: Compressing matrix to a short
# Maximize the numeric space. 
# 3 elements, mapped to 31 distinct values each.
result = (x1 * 32^2) + (x2 * 32^1) + (x3 * 32^0)
```
- **The Win:** It’s fast. Inference time is negligible ($~0.3$s).
    
- **The Loss:** Precision. We have to round values. The Mean Squared Error (MSE) was around 643 for a 16-bit conversion. It’s "good enough" for rough movement, but painful if you need precision.
    

### 2. The Memory Bank: Ordered Dictionary

If the robot plays enough games, it sees the same uncertainty patterns over and over. Why calculate when you can remember?

I built a dictionary of the most frequent matrices, ordered by occurrence. We cluster similar matrices (Euclidean distance) and just send the index of the match.

- **The Win:** Insane accuracy. MSE drops to near zero ($7.46$ at 16-bit).
    
- **The Loss:** It eats RAM. The robot has to store the map and search it every cycle. It’s computationally heavy.
    

### 3. The Black Box: Autoencoder

This was the "wabi-sabi" approach. Let a neural network figure out the messy representation. I used an autoencoder to compress the input layer (3 values) down to a bottleneck of 14 bits, then reconstruct it.

- **The Win:** It adapts. It doesn't need a pre-calculated dictionary of specific matches. It learned the _structure_ of the data.
    
- **The Stats:**
    
    - MSE: 13 (Very reliable).
        
    - Inference Time: 5.2s (Slower than math, faster than a massive dictionary search).
        

## Deployment

The results forced a hard look at what "optimization" actually means.

If I need pure speed and low CPU usage (which the NAO needs), the **Base Conversion** is the brutalist winner. It’s ugly, but it works instantly.

If I have memory to spare and need the robot to thread a needle, the **Dictionary** wins.

But the **Neural Network** felt like the most robust solution for the future. It balances the load and adapts to new environments without manual recalibration.

## Thoughts on the Edge

This research isn't just about robot soccer. It’s about Edge Computing. Whether it’s sensor networks or IoT, we are constantly trying to shove more data through smaller pipes.

Sometimes the best way to see the world clearly is to blur the details just enough to make them manageable.